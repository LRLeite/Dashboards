{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330b3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processo ETL...\n",
      "Conectado ao banco de dados 'brazilian_ecommerce' com sucesso.\n",
      "Banco de dados 'brazilian_ecommerce' já existe. Prosseguindo.\n",
      "Conectado ao banco de dados 'brazilian_ecommerce' com sucesso.\n",
      "Executando script SQL para dropar e criar tabelas...\n",
      "Tabelas criadas/redefinidas com sucesso.\n",
      "\n",
      "--- Carregando e limpando dados dos CSVs ---\n",
      "Processando product_category_name_translation.csv...\n",
      "Dados carregados para 'product_category_name_translation' com sucesso. (72 linhas)\n",
      "Processando olist_geolocation_dataset.csv...\n",
      "Dados carregados para 'geolocation' com sucesso. (720154 linhas)\n",
      "Populando 'zip_code_master' a partir de 'geolocation'...\n",
      "Tabela 'zip_code_master' populada com sucesso.\n",
      "Processando olist_customers_dataset.csv...\n",
      "Removidas 278 linhas de 'customers' devido a CEPs não encontrados em 'zip_code_master'.\n",
      "Dados carregados para 'customers' com sucesso. (99163 linhas)\n",
      "Processando olist_sellers_dataset.csv...\n",
      "Removidas 7 linhas de 'sellers' devido a CEPs não encontrados em 'zip_code_master'.\n",
      "Dados carregados para 'sellers' com sucesso. (3088 linhas)\n",
      "Processando olist_products_dataset.csv...\n",
      "Identificadas 1 categorias de produtos ausentes. Inserindo...\n",
      "Inseridas 1 categorias ausentes em 'product_category_name_translation'.\n",
      "Dados carregados para 'products' com sucesso. (32340 linhas)\n",
      "Processando olist_orders_dataset.csv...\n",
      "Removidas 278 linhas de 'orders' devido a customer_ids não encontrados em 'customers'.\n",
      "Dados carregados para 'orders' com sucesso. (99163 linhas)\n",
      "Processando olist_order_reviews_dataset.csv...\n",
      "Removidas 276 linhas de 'order_reviews' devido a order_ids não encontrados em 'orders'.\n",
      "Dados carregados para 'order_reviews' com sucesso. (98134 linhas)\n",
      "Processando olist_order_payments_dataset.csv...\n",
      "Removidas 287 linhas de 'order_payments' devido a order_ids não encontrados em 'orders'.\n",
      "Dados carregados para 'order_payments' com sucesso. (103599 linhas)\n",
      "Processando olist_order_items_dataset.csv...\n",
      "Removidas 2151 linhas de 'order_items' devido a FKs não encontradas.\n",
      "Dados carregados para 'order_items' com sucesso. (110499 linhas)\n",
      "\n",
      "--- Verificando Chaves Estrangeiras ---\n",
      "Verificações de chave estrangeira foram incorporadas na lógica de carregamento e no schema SQL.\n",
      "Conexão com o banco de dados fechada.\n",
      "\n",
      "Processo ETL concluído.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. CONFIGURAÇÕES ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "# ATENÇÃO: SUBSTITUA ESTAS CREDENCIAIS PELAS DO SEU BANCO DE DADOS PostgreSQL LOCAL\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',                 # Para PostgreSQL local\n",
    "    'port': '5432',                      # Porta padrão para PostgreSQL\n",
    "    'user': 'postgres',                  # Usuário padrão do PostgreSQL\n",
    "    'password': 'senha123', # <-- MUITO IMPORTANTE: SUBSTITUA PELA SUA SENHA REAL DO POSTGRES\n",
    "    'dbname': 'brazilian_ecommerce'      # Nome do banco de dados que será criado/usado\n",
    "}\n",
    "\n",
    "# Caminho base para os arquivos CSV no seu computador local\n",
    "# Ex: 'C:\\\\Users\\\\SeuUsuario\\\\Documentos\\\\DadosECommerce'\n",
    "# Certifique-se de que este caminho está correto no seu sistema\n",
    "CSV_BASE_PATH = 'D:\\\\Users\\\\LRL\\\\Desktop\\\\PBI Projects\\\\Brazilian e-commerce\\\\archive' # <-- AJUSTE ESTE CAMINHO\n",
    "\n",
    "CSV_FILES = {\n",
    "    'product_category_name_translation': 'product_category_name_translation.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'orders': 'olist_orders_dataset.csv',\n",
    "    'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'order_payments': 'olist_order_payments_dataset.csv',\n",
    "    'sellers': 'olist_sellers_dataset.csv',\n",
    "    'order_items': 'olist_order_items_dataset.csv',\n",
    "    'geolocation': 'olist_geolocation_dataset.csv',\n",
    "    'customers': 'olist_customers_dataset.csv'\n",
    "}\n",
    "\n",
    "# --- 2. SCHEMA SQL ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Este SQL é usado para criar as tabelas no PostgreSQL.\n",
    "# Inclui todas as correções de tipo de dados e definições de FK/PK.\n",
    "# A tabela 'zip_code_master' e sua população estão incluídas aqui.\n",
    "SCHEMA_SQL = \"\"\"\n",
    "-- Criação da tabela product_category_name_translation\n",
    "CREATE TABLE public.product_category_name_translation (\n",
    "    product_category_name TEXT PRIMARY KEY,\n",
    "    product_category_name_english TEXT\n",
    ");\n",
    "\n",
    "-- Criação da tabela products\n",
    "CREATE TABLE public.products (\n",
    "    product_id TEXT PRIMARY KEY,\n",
    "    product_category_name TEXT,\n",
    "    product_name_lenght BIGINT,\n",
    "    product_description_lenght BIGINT,\n",
    "    product_photos_qty BIGINT,\n",
    "    product_length_cm BIGINT,\n",
    "    product_height_cm BIGINT,\n",
    "    product_width_cm BIGINT,\n",
    "    product_weight_g BIGINT, -- Corrigido: product_weight_g estava faltando aqui\n",
    "    CONSTRAINT fk_product_category_name\n",
    "        FOREIGN KEY (product_category_name)\n",
    "        REFERENCES public.product_category_name_translation (product_category_name)\n",
    ");\n",
    "\n",
    "-- Criação da tabela orders\n",
    "CREATE TABLE public.orders (\n",
    "    order_id TEXT PRIMARY KEY,\n",
    "    customer_id TEXT NOT NULL,\n",
    "    order_status TEXT,\n",
    "    order_purchase_timestamp TIMESTAMP WITH TIME ZONE,\n",
    "    order_approved_at TIMESTAMP WITH TIME ZONE,\n",
    "    order_delivered_carrier_date TIMESTAMP WITH TIME ZONE,\n",
    "    order_delivered_customer_date TIMESTAMP WITH TIME ZONE,\n",
    "    order_estimated_delivery_date TIMESTAMP WITH TIME ZONE\n",
    ");\n",
    "\n",
    "-- Criação da tabela order_reviews\n",
    "CREATE TABLE public.order_reviews (\n",
    "    review_id TEXT PRIMARY KEY,\n",
    "    order_id TEXT NOT NULL,\n",
    "    review_score BIGINT,\n",
    "    review_comment_title TEXT,\n",
    "    review_comment_message TEXT,\n",
    "    review_creation_date TIMESTAMP WITH TIME ZONE,\n",
    "    review_answer_timestamp TIMESTAMP WITH TIME ZONE,\n",
    "    CONSTRAINT fk_order_id\n",
    "        FOREIGN KEY (order_id)\n",
    "        REFERENCES public.orders (order_id)\n",
    ");\n",
    "\n",
    "-- Criação da tabela order_payments\n",
    "CREATE TABLE public.order_payments (\n",
    "    order_id TEXT NOT NULL,\n",
    "    payment_sequential BIGINT NOT NULL,\n",
    "    payment_type TEXT,\n",
    "    payment_installments BIGINT,\n",
    "    payment_value DOUBLE PRECISION, -- CORRIGIDO: de BIGINT para DOUBLE PRECISION\n",
    "    PRIMARY KEY (order_id, payment_sequential),\n",
    "    CONSTRAINT fk_order_id\n",
    "        FOREIGN KEY (order_id)\n",
    "        REFERENCES public.orders (order_id)\n",
    ");\n",
    "\n",
    "-- Criação da tabela sellers\n",
    "CREATE TABLE public.sellers (\n",
    "    seller_id TEXT PRIMARY KEY,\n",
    "    seller_zip_code_prefix TEXT,\n",
    "    seller_city TEXT,\n",
    "    seller_state TEXT\n",
    ");\n",
    "\n",
    "-- Criação da tabela order_items\n",
    "CREATE TABLE public.order_items (\n",
    "    order_id TEXT NOT NULL,\n",
    "    order_item_id TEXT NOT NULL,\n",
    "    product_id TEXT NOT NULL,\n",
    "    seller_id TEXT NOT NULL,\n",
    "    shipping_limit_date TIMESTAMP WITH TIME ZONE,\n",
    "    price DOUBLE PRECISION,\n",
    "    freight_value DOUBLE PRECISION,\n",
    "    PRIMARY KEY (order_id, order_item_id),\n",
    "    CONSTRAINT fk_order_id\n",
    "        FOREIGN KEY (order_id)\n",
    "        REFERENCES public.orders (order_id),\n",
    "    CONSTRAINT fk_product_id\n",
    "        FOREIGN KEY (product_id)\n",
    "        REFERENCES public.products (product_id),\n",
    "    CONSTRAINT fk_seller_id\n",
    "        FOREIGN KEY (seller_id)\n",
    "        REFERENCES public.sellers (seller_id)\n",
    ");\n",
    "\n",
    "-- Criação da tabela geolocation\n",
    "CREATE TABLE public.geolocation (\n",
    "    geolocation_zip_code_prefix TEXT NOT NULL,\n",
    "    geolocation_lat DOUBLE PRECISION,\n",
    "    geolocation_lng DOUBLE PRECISION,\n",
    "    geolocation_city TEXT,\n",
    "    geolocation_state TEXT,\n",
    "    -- A PK composta garante unicidade, mas o zip_code_prefix sozinho não é único aqui.\n",
    "    -- A unicidade para FKs será tratada pela tabela zip_code_master.\n",
    "    PRIMARY KEY (geolocation_zip_code_prefix, geolocation_lat, geolocation_lng)\n",
    ");\n",
    "\n",
    "-- Criação da nova tabela intermediária 'zip_code_master'\n",
    "-- Esta tabela terá um registro único para cada CEP e suas coordenadas representativas.\n",
    "CREATE TABLE public.zip_code_master (\n",
    "    zip_code_prefix TEXT PRIMARY KEY,\n",
    "    latitude DOUBLE PRECISION,\n",
    "    longitude DOUBLE PRECISION,\n",
    "    city TEXT,\n",
    "    state TEXT\n",
    ");\n",
    "\n",
    "-- Criação da tabela customers\n",
    "CREATE TABLE public.customers (\n",
    "    customer_id TEXT PRIMARY KEY,\n",
    "    customer_unique_id TEXT,\n",
    "    customer_zip_code_prefix TEXT,\n",
    "    customer_city TEXT,\n",
    "    customer_state TEXT,\n",
    "    CONSTRAINT fk_customer_zip_code_prefix\n",
    "        FOREIGN KEY (customer_zip_code_prefix)\n",
    "        REFERENCES public.zip_code_master (zip_code_prefix) -- Referencia a nova tabela zip_code_master\n",
    ");\n",
    "\n",
    "-- Adição de Chaves Estrangeiras pendentes em orders\n",
    "ALTER TABLE public.orders\n",
    "ADD CONSTRAINT fk_customer_id\n",
    "    FOREIGN KEY (customer_id)\n",
    "    REFERENCES public.customers (customer_id);\n",
    "\n",
    "-- Adição de Chaves Estrangeiras pendentes em sellers (referenciando zip_code_master)\n",
    "ALTER TABLE public.sellers\n",
    "ADD CONSTRAINT fk_seller_zip_code_prefix\n",
    "    FOREIGN KEY (seller_zip_code_prefix)\n",
    "    REFERENCES public.zip_code_master (zip_code_prefix);\n",
    "\"\"\"\n",
    "\n",
    "# SQL para popular a tabela zip_code_master (executado após carregar 'geolocation')\n",
    "POPULATE_ZIP_CODE_MASTER_SQL = \"\"\"\n",
    "INSERT INTO public.zip_code_master (zip_code_prefix, latitude, longitude, city, state)\n",
    "SELECT DISTINCT ON (geolocation_zip_code_prefix)\n",
    "       geolocation_zip_code_prefix,\n",
    "       geolocation_lat,\n",
    "       geolocation_lng,\n",
    "       geolocation_city,\n",
    "       geolocation_state\n",
    "FROM public.geolocation\n",
    "ORDER BY geolocation_zip_code_prefix, geolocation_lat, geolocation_lng;\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. FUNÇÕES DE ETL ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def connect_db(dbname='postgres'): # Conecta ao banco de dados padrão 'postgres' para criar o DB principal\n",
    "    \"\"\"Conecta ao banco de dados PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_CONFIG['host'],\n",
    "            port=DB_CONFIG['port'],\n",
    "            user=DB_CONFIG['user'],\n",
    "            password=DB_CONFIG['password'],\n",
    "            dbname=dbname\n",
    "        )\n",
    "        conn.autocommit = True # Para comandos como CREATE DATABASE\n",
    "        print(f\"Conectado ao banco de dados '{dbname}' com sucesso.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao conectar ao banco de dados '{dbname}': {e}\")\n",
    "        return None\n",
    "\n",
    "def create_main_database_if_not_exists():\n",
    "    \"\"\"Cria o banco de dados principal (brazilian_ecommerce) se ele não existir.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # Tenta conectar ao banco de dados principal para verificar se existe\n",
    "        conn = connect_db(dbname=DB_CONFIG['dbname'])\n",
    "        if conn:\n",
    "            print(f\"Banco de dados '{DB_CONFIG['dbname']}' já existe. Prosseguindo.\")\n",
    "            conn.close()\n",
    "            return\n",
    "        \n",
    "        # Se a conexão direta falhou (DB não existe ou erro de conexão), tenta criar\n",
    "        # Conecta ao banco de dados padrão 'postgres' para criar um novo DB\n",
    "        conn_postgres = connect_db(dbname='postgres')\n",
    "        if conn_postgres:\n",
    "            cur = conn_postgres.cursor()\n",
    "            print(f\"Criando banco de dados '{DB_CONFIG['dbname']}'...\")\n",
    "            cur.execute(f\"CREATE DATABASE {DB_CONFIG['dbname']} WITH OWNER = {DB_CONFIG['user']} ENCODING = 'UTF8' LC_COLLATE = 'C' LC_CTYPE = 'C' TEMPLATE = template0 CONNECTION LIMIT = -1;\")\n",
    "            print(f\"Banco de dados '{DB_CONFIG['dbname']}' criado com sucesso.\")\n",
    "            cur.close()\n",
    "            conn_postgres.close()\n",
    "        else:\n",
    "            print(\"Não foi possível conectar ao banco de dados 'postgres' para criar o DB principal. Verifique as credenciais e o status do servidor.\")\n",
    "            raise Exception(\"Falha na conexão inicial ao PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar/verificar banco de dados principal: {e}\")\n",
    "        raise # Re-lança o erro para parar o processo ETL\n",
    "\n",
    "def setup_tables(conn):\n",
    "    \"\"\"Executa o script SQL para dropar e criar as tabelas.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        print(\"Executando script SQL para dropar e criar tabelas...\")\n",
    "        cur.execute(SCHEMA_SQL)\n",
    "        conn.commit()\n",
    "        print(\"Tabelas criadas/redefinidas com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar tabelas: {e}\")\n",
    "        conn.rollback() # Reverte em caso de erro\n",
    "        raise # Re-lança o erro para parar o processo ETL\n",
    "\n",
    "def load_data_to_db(df, table_name, conn):\n",
    "    \"\"\"Carrega um DataFrame para uma tabela PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        # Convert NaT (Not a Time) to None for datetime columns\n",
    "        for col in df.select_dtypes(include=['datetime64[ns, UTC]', 'datetime64[ns]']).columns:\n",
    "            df[col] = df[col].replace({pd.NaT: None})\n",
    "\n",
    "        # Substitui NaN (Not a Number) por None para compatibilidade com NULL do PostgreSQL\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "        cols = ','.join([f'\"{c}\"' for c in list(df.columns)]) # Adiciona aspas para nomes de colunas\n",
    "        \n",
    "        query = f\"INSERT INTO public.{table_name}({cols}) VALUES %s;\"\n",
    "        cur = conn.cursor()\n",
    "        extras.execute_values(cur, query, tuples, page_size=10000)\n",
    "        conn.commit()\n",
    "        print(f\"Dados carregados para '{table_name}' com sucesso. ({len(df)} linhas)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados para '{table_name}': {e}\")\n",
    "        conn.rollback()\n",
    "        raise # Re-lança o erro para parar o processo ETL\n",
    "\n",
    "def get_existing_categories(conn):\n",
    "    \"\"\"Obtém as categorias existentes na tabela product_category_name_translation.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT product_category_name FROM public.product_category_name_translation;\")\n",
    "        existing_categories = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_categories\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter categorias existentes: {e}\")\n",
    "        return set()\n",
    "\n",
    "def insert_missing_categories(conn, missing_categories_df):\n",
    "    \"\"\"Insere categorias ausentes na tabela product_category_name_translation.\"\"\"\n",
    "    if missing_categories_df.empty:\n",
    "        return\n",
    "    try:\n",
    "        tuples = [tuple(x) for x in missing_categories_df.to_numpy()]\n",
    "        cols = ','.join([f'\"{c}\"' for c in list(missing_categories_df.columns)])\n",
    "        query = f\"INSERT INTO public.product_category_name_translation({cols}) VALUES %s ON CONFLICT (product_category_name) DO NOTHING;\"\n",
    "        cur = conn.cursor()\n",
    "        extras.execute_values(cur, query, tuples, page_size=1000)\n",
    "        conn.commit()\n",
    "        print(f\"Inseridas {len(missing_categories_df)} categorias ausentes em 'product_category_name_translation'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inserir categorias ausentes: {e}\")\n",
    "        conn.rollback()\n",
    "        raise # Re-lança o erro\n",
    "\n",
    "def get_existing_zip_codes(conn):\n",
    "    \"\"\"Obtém os CEPs existentes na tabela zip_code_master.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT zip_code_prefix FROM public.zip_code_master;\")\n",
    "        existing_zips = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_zips\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter CEPs existentes da zip_code_master: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_existing_customer_ids(conn):\n",
    "    \"\"\"Obtém os customer_ids existentes na tabela customers.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT customer_id FROM public.customers;\")\n",
    "        existing_customer_ids = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_customer_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter customer_ids existentes: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_existing_product_ids(conn):\n",
    "    \"\"\"Obtém os product_ids existentes na tabela products.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT product_id FROM public.products;\")\n",
    "        existing_product_ids = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_product_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter product_ids existentes: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_existing_seller_ids(conn):\n",
    "    \"\"\"Obtém os seller_ids existentes na tabela sellers.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT seller_id FROM public.sellers;\")\n",
    "        existing_seller_ids = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_seller_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter seller_ids existentes: {e}\")\n",
    "        return set()\n",
    "\n",
    "def get_existing_order_ids(conn):\n",
    "    \"\"\"Obtém os order_ids existentes na tabela orders.\"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT order_id FROM public.orders;\")\n",
    "        existing_order_ids = {row[0] for row in cur.fetchall()}\n",
    "        cur.close()\n",
    "        return existing_order_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter order_ids existentes: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- 4. PROCESSO PRINCIPAL DE ETL ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def run_etl():\n",
    "    \"\"\"Orquestra o processo ETL completo.\"\"\"\n",
    "    print(\"Iniciando processo ETL...\")\n",
    "\n",
    "    # 1. Criar banco de dados principal (se necessário)\n",
    "    try:\n",
    "        create_main_database_if_not_exists()\n",
    "    except Exception as e:\n",
    "        print(f\"Falha na etapa de criação do banco de dados: {e}\")\n",
    "        print(\"Encerrando ETL.\")\n",
    "        return\n",
    "\n",
    "    # 2. Conectar ao banco de dados principal para operações de schema e dados\n",
    "    conn = connect_db(dbname=DB_CONFIG['dbname'])\n",
    "    if not conn:\n",
    "        print(\"Não foi possível conectar ao banco de dados principal. Encerrando ETL.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 3. Configurar tabelas (dropar e criar)\n",
    "        setup_tables(conn)\n",
    "\n",
    "        # Dicionário para armazenar DataFrames limpos (não estritamente necessário para este fluxo, mas útil para depuração)\n",
    "        cleaned_dfs = {}\n",
    "\n",
    "        # --- Carregamento e Limpeza de Dados ---\n",
    "        print(\"\\n--- Carregando e limpando dados dos CSVs ---\")\n",
    "\n",
    "        # 1. product_category_name_translation\n",
    "        print(f\"Processando {CSV_FILES['product_category_name_translation']}...\")\n",
    "        df_cat_trans = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['product_category_name_translation']), encoding='utf-8')\n",
    "        df_cat_trans.columns = df_cat_trans.columns.str.strip() # Remover espaços em branco nos nomes das colunas\n",
    "        df_cat_trans['product_category_name'] = df_cat_trans['product_category_name'].astype(str).str.strip()\n",
    "        df_cat_trans['product_category_name_english'] = df_cat_trans['product_category_name_english'].astype(str).str.strip()\n",
    "        df_cat_trans.drop_duplicates(subset=['product_category_name'], inplace=True) # Garantir PK única\n",
    "        load_data_to_db(df_cat_trans, 'product_category_name_translation', conn)\n",
    "        cleaned_dfs['product_category_name_translation'] = df_cat_trans # Armazena para referência futura\n",
    "\n",
    "        # 2. geolocation\n",
    "        print(f\"Processando {CSV_FILES['geolocation']}...\")\n",
    "        df_geo = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['geolocation']), encoding='utf-8')\n",
    "        df_geo.columns = df_geo.columns.str.strip()\n",
    "        # Correção de tipo de dados para lat/lng (garantir que são numéricos)\n",
    "        df_geo['geolocation_lat'] = pd.to_numeric(df_geo['geolocation_lat'], errors='coerce')\n",
    "        df_geo['geolocation_lng'] = pd.to_numeric(df_geo['geolocation_lng'], errors='coerce')\n",
    "        df_geo.dropna(subset=['geolocation_lat', 'geolocation_lng'], inplace=True) # Remover linhas com lat/lng nulos após coerção\n",
    "        # Remover duplicatas da PK composta (zip, lat, lng)\n",
    "        df_geo.drop_duplicates(subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], inplace=True)\n",
    "        load_data_to_db(df_geo, 'geolocation', conn)\n",
    "        cleaned_dfs['geolocation'] = df_geo\n",
    "\n",
    "        # 3. Popular zip_code_master a partir de geolocation (executado via SQL)\n",
    "        print(\"Populando 'zip_code_master' a partir de 'geolocation'...\")\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(POPULATE_ZIP_CODE_MASTER_SQL)\n",
    "        conn.commit()\n",
    "        print(\"Tabela 'zip_code_master' populada com sucesso.\")\n",
    "\n",
    "        # 4. customers (depende de zip_code_master) - MOVIDO PARA CIMA\n",
    "        print(f\"Processando {CSV_FILES['customers']}...\")\n",
    "        df_customers = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['customers']), encoding='utf-8')\n",
    "        df_customers.columns = df_customers.columns.str.strip()\n",
    "        df_customers.drop_duplicates(subset=['customer_id'], inplace=True) # Garantir PK única\n",
    "\n",
    "        # Filtrar clientes com CEPs não existentes em zip_code_master\n",
    "        existing_zips = get_existing_zip_codes(conn)\n",
    "        initial_customer_count = len(df_customers)\n",
    "        df_customers = df_customers[df_customers['customer_zip_code_prefix'].astype(str).isin(existing_zips)] # Cast to str for comparison\n",
    "        if len(df_customers) < initial_customer_count:\n",
    "            print(f\"Removidas {initial_customer_count - len(df_customers)} linhas de 'customers' devido a CEPs não encontrados em 'zip_code_master'.\")\n",
    "        load_data_to_db(df_customers, 'customers', conn)\n",
    "        cleaned_dfs['customers'] = df_customers\n",
    "\n",
    "        # 5. sellers (depende de zip_code_master) - MOVIDO PARA CIMA\n",
    "        print(f\"Processando {CSV_FILES['sellers']}...\")\n",
    "        df_sellers = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['sellers']), encoding='utf-8')\n",
    "        df_sellers.columns = df_sellers.columns.str.strip()\n",
    "        df_sellers.drop_duplicates(subset=['seller_id'], inplace=True) # Garantir PK única\n",
    "\n",
    "        # Filtrar vendedores com CEPs não existentes em zip_code_master\n",
    "        initial_seller_count = len(df_sellers)\n",
    "        df_sellers = df_sellers[df_sellers['seller_zip_code_prefix'].astype(str).isin(existing_zips)] # Cast to str for comparison\n",
    "        if len(df_sellers) < initial_seller_count:\n",
    "            print(f\"Removidas {initial_seller_count - len(df_sellers)} linhas de 'sellers' devido a CEPs não encontrados em 'zip_code_master'.\")\n",
    "        load_data_to_db(df_sellers, 'sellers', conn)\n",
    "        cleaned_dfs['sellers'] = df_sellers\n",
    "\n",
    "        # 6. products (depende de product_category_name_translation) - ORDEM MANTIDA\n",
    "        print(f\"Processando {CSV_FILES['products']}...\")\n",
    "        df_products = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['products']), encoding='utf-8')\n",
    "        df_products.columns = df_products.columns.str.strip()\n",
    "        df_products['product_category_name'] = df_products['product_category_name'].astype(str).str.strip() # Remove espaços em branco\n",
    "\n",
    "        # CORREÇÃO: Converter colunas BIGINT para numérico e tratar NaNs\n",
    "        bigint_cols_products = [\n",
    "            'product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "            'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm'\n",
    "        ]\n",
    "        for col in bigint_cols_products:\n",
    "            df_products[col] = pd.to_numeric(df_products[col], errors='coerce').astype('Int64') # Use 'Int64' para suportar NaNs\n",
    "        df_products.dropna(subset=bigint_cols_products, inplace=True) # Remover linhas onde a conversão falhou\n",
    "\n",
    "        df_products.drop_duplicates(subset=['product_id'], inplace=True) # Garantir PK única\n",
    "\n",
    "        # Lidar com categorias de produtos ausentes em product_category_name_translation\n",
    "        existing_categories = get_existing_categories(conn)\n",
    "        product_categories = set(df_products['product_category_name'].dropna().unique())\n",
    "        missing_categories = product_categories - existing_categories\n",
    "\n",
    "        if missing_categories:\n",
    "            print(f\"Identificadas {len(missing_categories)} categorias de produtos ausentes. Inserindo...\")\n",
    "            # Cria um DataFrame para as categorias ausentes (sem tradução, usa o próprio nome)\n",
    "            missing_cat_df = pd.DataFrame({\n",
    "                'product_category_name': list(missing_categories),\n",
    "                'product_category_name_english': list(missing_categories) # Usa o próprio nome como tradução padrão\n",
    "            })\n",
    "            insert_missing_categories(conn, missing_cat_df)\n",
    "        else:\n",
    "            print(\"Nenhuma categoria de produto ausente identificada.\")\n",
    "\n",
    "        load_data_to_db(df_products, 'products', conn)\n",
    "        cleaned_dfs['products'] = df_products\n",
    "\n",
    "        # 7. orders (depende de customers) - AGORA VEM DEPOIS DE CUSTOMERS\n",
    "        print(f\"Processando {CSV_FILES['orders']}...\")\n",
    "        df_orders = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['orders']), encoding='utf-8')\n",
    "        df_orders.columns = df_orders.columns.str.strip()\n",
    "        # Converter colunas de data/hora\n",
    "        datetime_cols_orders = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "        for col in datetime_cols_orders:\n",
    "            df_orders[col] = pd.to_datetime(df_orders[col], errors='coerce', utc=True) # errors='coerce' para NaN em erros\n",
    "        df_orders.drop_duplicates(subset=['order_id'], inplace=True) # Garantir PK única\n",
    "\n",
    "        # NOVO: Filtrar orders com customer_id não existentes em customers\n",
    "        existing_customer_ids = get_existing_customer_ids(conn) # Obtém os IDs de clientes que foram carregados\n",
    "        initial_order_count = len(df_orders)\n",
    "        df_orders = df_orders[df_orders['customer_id'].astype(str).isin(existing_customer_ids)]\n",
    "        if len(df_orders) < initial_order_count:\n",
    "            print(f\"Removidas {initial_order_count - len(df_orders)} linhas de 'orders' devido a customer_ids não encontrados em 'customers'.\")\n",
    "\n",
    "        load_data_to_db(df_orders, 'orders', conn)\n",
    "        cleaned_dfs['orders'] = df_orders\n",
    "\n",
    "        # 8. order_reviews (depende de orders)\n",
    "        print(f\"Processando {CSV_FILES['order_reviews']}...\")\n",
    "        df_reviews = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['order_reviews']), encoding='utf-8')\n",
    "        df_reviews.columns = df_reviews.columns.str.strip()\n",
    "        df_reviews.drop_duplicates(subset=['review_id'], inplace=True) # Remover duplicatas da PK\n",
    "        # Converter colunas de data/hora\n",
    "        datetime_cols_reviews = ['review_creation_date', 'review_answer_timestamp']\n",
    "        for col in datetime_cols_reviews:\n",
    "            df_reviews[col] = pd.to_datetime(df_reviews[col], errors='coerce', utc=True)\n",
    "        \n",
    "        # NOVO: Filtrar order_reviews com order_id não existentes em orders\n",
    "        existing_order_ids = get_existing_order_ids(conn) # Obtém os IDs de pedidos que foram carregados\n",
    "        initial_review_count = len(df_reviews)\n",
    "        df_reviews = df_reviews[df_reviews['order_id'].astype(str).isin(existing_order_ids)]\n",
    "        if len(df_reviews) < initial_review_count:\n",
    "            print(f\"Removidas {initial_review_count - len(df_reviews)} linhas de 'order_reviews' devido a order_ids não encontrados em 'orders'.\")\n",
    "\n",
    "        load_data_to_db(df_reviews, 'order_reviews', conn)\n",
    "        cleaned_dfs['order_reviews'] = df_reviews\n",
    "\n",
    "        # 9. order_payments (depende de orders)\n",
    "        print(f\"Processando {CSV_FILES['order_payments']}...\")\n",
    "        df_payments = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['order_payments']), encoding='utf-8')\n",
    "        df_payments.columns = df_payments.columns.str.strip()\n",
    "        # Correção de tipo de dados para payment_value\n",
    "        df_payments['payment_value'] = pd.to_numeric(df_payments['payment_value'], errors='coerce')\n",
    "        df_payments.dropna(subset=['payment_value'], inplace=True) # Remover linhas com payment_value nulo após coerção\n",
    "        # PK composta (order_id, payment_sequential)\n",
    "        df_payments.drop_duplicates(subset=['order_id', 'payment_sequential'], inplace=True)\n",
    "\n",
    "        # NOVO: Filtrar order_payments com order_id não existentes em orders\n",
    "        # existing_order_ids já foi obtido para order_reviews\n",
    "        initial_payment_count = len(df_payments)\n",
    "        df_payments = df_payments[df_payments['order_id'].astype(str).isin(existing_order_ids)]\n",
    "        if len(df_payments) < initial_payment_count:\n",
    "            print(f\"Removidas {initial_payment_count - len(df_payments)} linhas de 'order_payments' devido a order_ids não encontrados em 'orders'.\")\n",
    "\n",
    "        load_data_to_db(df_payments, 'order_payments', conn)\n",
    "        cleaned_dfs['order_payments'] = df_payments\n",
    "\n",
    "        # 10. order_items (depende de orders, products, sellers)\n",
    "        print(f\"Processando {CSV_FILES['order_items']}...\")\n",
    "        df_items = pd.read_csv(os.path.join(CSV_BASE_PATH, CSV_FILES['order_items']), encoding='utf-8')\n",
    "        df_items.columns = df_items.columns.str.strip()\n",
    "        # Correção de tipo de dados para price e freight_value\n",
    "        df_items['price'] = pd.to_numeric(df_items['price'], errors='coerce')\n",
    "        df_items['freight_value'] = pd.to_numeric(df_items['freight_value'], errors='coerce')\n",
    "        df_items.dropna(subset=['price', 'freight_value'], inplace=True) # Remover linhas com valores nulos após coerção\n",
    "        # Converter coluna de data/hora\n",
    "        df_items['shipping_limit_date'] = pd.to_datetime(df_items['shipping_limit_date'], errors='coerce', utc=True)\n",
    "        # PK composta (order_id, order_item_id)\n",
    "        df_items.drop_duplicates(subset=['order_id', 'order_item_id'], inplace=True)\n",
    "\n",
    "        # NOVO: Filtrar order_items com FKs não existentes\n",
    "        # existing_order_ids já foi obtido\n",
    "        existing_product_ids = get_existing_product_ids(conn) # Obtém os IDs de produtos que foram carregados\n",
    "        existing_seller_ids = get_existing_seller_ids(conn)   # Obtém os IDs de vendedores que foram carregados\n",
    "\n",
    "        initial_item_count = len(df_items)\n",
    "        df_items = df_items[df_items['order_id'].astype(str).isin(existing_order_ids)]\n",
    "        df_items = df_items[df_items['product_id'].astype(str).isin(existing_product_ids)]\n",
    "        df_items = df_items[df_items['seller_id'].astype(str).isin(existing_seller_ids)]\n",
    "\n",
    "        if len(df_items) < initial_item_count:\n",
    "            print(f\"Removidas {initial_item_count - len(df_items)} linhas de 'order_items' devido a FKs não encontradas.\")\n",
    "\n",
    "        load_data_to_db(df_items, 'order_items', conn)\n",
    "        cleaned_dfs['order_items'] = df_items\n",
    "\n",
    "\n",
    "        print(\"\\n--- Verificando Chaves Estrangeiras ---\")\n",
    "        print(\"Verificações de chave estrangeira foram incorporadas na lógica de carregamento e no schema SQL.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRO FATAL NO ETL: {e}\")\n",
    "        print(\"Processo ETL encerrado devido a erro.\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Conexão com o banco de dados fechada.\")\n",
    "    print(\"\\nProcesso ETL concluído.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_etl()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
